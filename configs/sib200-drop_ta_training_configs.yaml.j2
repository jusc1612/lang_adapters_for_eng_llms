defaults: &default_values
  hf_token: "{{ env('HF_TOKEN', None) }}"
  cache_dir: "{{ env('CACHE_DIR', None) }}"
  output_dir: "{{ env('OUTPUT_DIR', None) }}"
  ta_path: 'ta_sib200-drop'
  mode: 'train'
  do_train: ''
  overwrite_output_dir: ''
  ddp_backend: 'nccl'
  task: 'question-answering'
  task_dataset: 'MLQA-en'
  task_dataset_split: 'sentence_split'
  log_level: 'info'
  report_to: 'wandb'
  data_seed: 42
  do_eval: ''
  eval_strategy: 'steps'
  save_total_limit: 3
  load_best_model_at_end: ''

configs:

# 0
- <<: *default_values
  model_name_or_path: 'meta-llama/Llama-2-7b-hf'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'english'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  la_name: 'Llama-2-7b-hf_cc100_en_12500'
  ta_name: 'llama-2_bn_cc100_en_sib200'

# 1
- <<: *default_values
  model_name_or_path: 'meta-llama/Llama-2-7b-hf'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'german'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  la_name: 'Llama-2-7b-hf_cc100_de_12500'
  ta_name: 'llama-2_bn_cc100_de_sib200'

# 2
- <<: *default_values
  model_name_or_path: 'meta-llama/Llama-2-7b-hf'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'spanish'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  la_name: 'Llama-2-7b-hf_cc100_es_12500'
  ta_name: 'llama-2_bn_cc100_es_sib200'

# 3
- <<: *default_values
  model_name_or_path: 'meta-llama/Llama-2-7b-hf'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'english'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  la_name: 'Llama-2-7b-hf_cc100_en_lora-alpha'
  ta_name: 'llama-2_lora_cc100_en_sib200'
  merge_weights: ''

# 4
- <<: *default_values
  model_name_or_path: 'meta-llama/Llama-2-7b-hf'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'german'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  la_name: 'Llama-2-7b-hf_cc100_de_lora-alpha'
  ta_name: 'llama-2_lora_cc100_de_sib200'
  merge_weights: ''

# 5
- <<: *default_values
  model_name_or_path: 'meta-llama/Llama-2-7b-hf'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'spanish'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  la_name: 'Llama-2-7b-hf_cc100_es_lora-alpha'
  ta_name: 'llama-2_lora_cc100_es_sib200'
  merge_weights: ''

# 6
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3-8B'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'english'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  la_name: 'Meta-Llama-3-8B_cc100_en_12500'
  ta_name: 'llama-3_bn_cc100_en_sib200'

# 7
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3-8B'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'german'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  la_name: 'Meta-Llama-3-8B_cc100_de_12500'
  ta_name: 'llama-3_bn_cc100_de_sib200'

# 8
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3-8B'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'spanish'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  la_name: 'Meta-Llama-3-8B_cc100_es_12500'
  ta_name: 'llama-3_bn_cc100_es_sib200'

# 9
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3-8B'
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  ta_languages: 'english'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  la_name: 'Meta-Llama-3-8B_cc100_en_lora-alpha'
  ta_name: 'llama-3_lora_cc100_en_sib200'
  merge_weights: ''

# 10
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3-8B'
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  ta_languages: 'german'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  la_name: 'Meta-Llama-3-8B_cc100_de_lora-alpha'
  ta_name: 'llama-3_lora_cc100_de_sib200'
  merge_weights: ''

# 11
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3-8B'
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  ta_languages: 'spanish'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  la_name: 'Meta-Llama-3-8B_cc100_es_lora-alpha'
  ta_name: 'llama-3_lora_cc100_es_sib200'
  merge_weights: ''

# 12
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3.1-8B'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'english'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  la_name: 'Meta-Llama-31-8B_cc100_en'
  ta_name: 'llama-31_bn_cc100_en_sib200'

# 13
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3.1-8B'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'german'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  la_name: 'Meta-Llama-31-8B_cc100_de'
  ta_name: 'llama-31_bn_cc100_de_sib200'

# 14
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3.1-8B'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'spanish'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  la_name: 'Meta-Llama-31-8B_cc100_es'
  ta_name: 'llama-31_bn_cc100_es_sib200'

# 15
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3.1-8B'
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  ta_languages: 'english'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  la_name: 'Meta-Llama-31-8B_cc100_en_lora-alpha'
  ta_name: 'llama-31_lora_cc100_en_sib200'
  merge_weights: ''

# 16
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3.1-8B'
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  ta_languages: 'german'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  la_name: 'Meta-Llama-31-8B_cc100_de_lora-alpha'
  ta_name: 'llama-31_lora_cc100_de_sib200'
  merge_weights: ''

# 17
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3.1-8B'
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  ta_languages: 'spanish'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  la_name: 'Meta-Llama-31-8B_cc100_es_lora-alpha'
  ta_name: 'llama-31_lora_cc100_es_sib200'
  merge_weights: ''

# 18
- <<: *default_values
  model_name_or_path: 'meta-llama/Llama-2-7b-hf'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'english'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  ta_name: 'llama-2_bn_en_sib200'

# 19
- <<: *default_values
  model_name_or_path: 'meta-llama/Llama-2-7b-hf'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'german'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  ta_name: 'llama-2_bn_de_sib200'

# 20
- <<: *default_values
  model_name_or_path: 'meta-llama/Llama-2-7b-hf'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'spanish'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  ta_name: 'llama-2_bn_es_sib200'

# 21
- <<: *default_values
  model_name_or_path: 'meta-llama/Llama-2-7b-hf'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'english'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  ta_name: 'llama-2_lora_en_sib200'

# 22
- <<: *default_values
  model_name_or_path: 'meta-llama/Llama-2-7b-hf'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'german'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  ta_name: 'llama-2_lora_de_sib200'

# 23
- <<: *default_values
  model_name_or_path: 'meta-llama/Llama-2-7b-hf'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'spanish'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  ta_name: 'llama-2_lora_es_sib200'

# 24
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3-8B'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'english'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  ta_name: 'llama-3_bn_en_sib200'

# 25
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3-8B'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'german'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  ta_name: 'llama-3_bn_de_sib200'

# 26
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3-8B'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'spanish'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  ta_name: 'llama-3_bn_es_sib200'

# 27
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3-8B'
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  ta_languages: 'english'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  ta_name: 'llama-3_lora_en_sib200'

# 28
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3-8B'
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  ta_languages: 'german'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  ta_name: 'llama-3_lora_de_sib200'

# 29
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3-8B'
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  ta_languages: 'spanish'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  ta_name: 'llama-3_lora_es_sib200'

# 30
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3.1-8B'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'english'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  ta_name: 'llama-31_bn_en_sib200'

# 31
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3.1-8B'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'german'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  ta_name: 'llama-31_bn_de_sib200'

# 32
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3.1-8B'
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  ta_languages: 'spanish'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  adapter_config: 'seq_bn'
  train_adapter: ''
  reduction_factor: 32
  adapter_dropout: 0.1
  ta_name: 'llama-31_bn_es_sib200'

# 33
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3.1-8B'
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  ta_languages: 'english'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  ta_name: 'llama-31_lora_en_sib200'

# 34
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3.1-8B'
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  ta_languages: 'german'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  ta_name: 'llama-31_lora_de_sib200'

# 35
- <<: *default_values
  model_name_or_path: 'meta-llama/Meta-Llama-3.1-8B'
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  ta_languages: 'spanish'
  lang_ratio: '1.0'
  dataset_name: 'Davlan/sib200'
  num_train_epochs: 20
  english_format: ''
  source_prompt: 'english'
  save_steps: 20
  eval_steps: 20
  logging_steps: 20
  peft_type: 'LORA'
  task_type: 'CAUSAL_LM'
  lora_rank: 64
  lora_alpha: 32
  adapter_dropout: 0.1
  attn_matrices: 'q_proj v_proj k_proj'
  ta_name: 'llama-31_lora_es_sib200'
